---
title: "Statistique computationnelle"
subtitle: "Méthodes d'échantillonnage"
author: Vadim Bertrand
date: '`r Sys.setlocale("LC_TIME", "fr_FR.UTF-8"); format(Sys.Date(), "%d %B %Y")`'
urlcolor: blue
linkcolor: blue
output:
  bookdown::pdf_document2: default
  toc: true
  toc_depth: 3
  number_section: true
  highlight: tango
toc-title: "Sommaire"
editor_options:
  markdown:
    wrap: sentence
header-includes:
    \usepackage{caption}
    \usepackage{mathtools}
    \usepackage{float}
    \makeatletter\renewcommand*{\fps@figure}{H}\makeatother
---

```{r setup, echo = FALSE, eval = TRUE, purl = FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE,
                      comment = NA, warning = FALSE,
                      message = FALSE)
knitr::opts_chunk$set(fig.width = 8, fig.height = 4,
                      fig.align = "center")
options(scipen=999)
```

```{r}
library(class)
library(dplyr)
library(ggplot2)
library(latex2exp)
library(patchwork)
library(pROC)
library(tibble)
library(tidyr)
```

```{r}
set.seed(15)
```

\newpage

# Exercice 1

```{r, Exercice1}
g <- function (x) { # définition de g(x)
  exp(-abs(x)) / 2
}
```

1.

$g(x)$ est une densité ssi :

   - $g(x)$ est $C^0$ et positive sur $\mathbb{R}$
   - $\int_{-\infty}^{+\infty} g(x) dx = 1$

Or, $exp(u)$ est $C^0$ et positive sur $\mathbb{R}^-$ (car elle l'est sur $\mathbb{R}$), donc $g(x) = \frac{1}{2} exp(-|x|)$ est $C^0$ et positive sur $\mathbb{R}$.
La figure \@ref(fig:plot-1-g-density) permet d'illustrer ces propriétés pour $x \in [-5, 5]$.

```{r, 1.1}
```

```{r plot-1-g-density, fig.cap = "Aperçu de la fonction $g(x) = \\frac{1}{2} exp(-|x|)$, pour $x \\in [-5, 5]$"}
ggplot() +
  xlim(-5, 5) +
  geom_function(fun = g) +
  labs(x = "x", y = "y")
```

Et d'après le développement \@ref(eq:ex1-1), nous avons bien que $\int_{-\infty}^{+\infty} g(x)dx = 1$ :

\begin{equation} (\#eq:ex1-1)
    \begin{split}
        \int_{-\infty}^{+\infty} g(x)dx
        & = \int_{-\infty}^{+\infty} \frac{1}{2} exp(-|x|) dx \\
        & = \frac{1}{2} \left( \int_{-\infty}^0 exp(x) dx + \int_0^{+\infty} exp(-x) dx \right) \\
        & = \frac{1}{2} \left[ [exp(x)]_{-\infty}^0 + [-exp(-x)]_0^{+\infty} \right] \\
        & = \frac{1}{2} (1 + 1) = 1
    \end{split}
\end{equation}

Ce qui équivaut à vérifier que $g(x)$ est bien une densité.

2.

La méthode d'inversion consiste à simuler un échantillon distribué selon une densité $d$ par le biais du tirage d'un échantillon suivant la loi uniforme $\mathcal{U}[0,1]$, permettant ensuite de revenir à la distribution initialement souhaitée ($d$) via la réciproque de la fonction de répartition de cette densité (fonction quantile).

Soient $G(x)$ la fonction de répartition de $g(x)$ et $G^{-1}(y)$ sa réciproque. \
Par définition, nous avons : $G(x)= \int_{-\infty}^{x} g(u) du$ et $(G^{-1} \circ G)(x) = x$.

Le développement \@ref(eq:ex1-2-1) donne l'expression de $G(x)$ pour $x \in \mathbb{R}^+$ et $x \in \mathbb{R}^-$.

\begin{subequations} (\#eq:ex1-2-1)
    \begin{align}
        \begin{split}
            \forall x \in \mathbb{R}^+, \qquad \qquad G_+(x)
            & = \frac{1}{2} \left( \int_{-\infty}^0 exp(u) du + \int_0^{x} exp(-u) du \right) \\
            & = \frac{1}{2} \left[ [exp(u)]_{-\infty}^0 + [-exp(-u)]_0^{x} \right] \\
            & = 1 - \frac{1}{2} exp(-x)
        \end{split} \\
        \begin{split}
            \forall x \in \mathbb{R}^-, \qquad \qquad G_-(x)
            & = \frac{1}{2} \int_{-\infty}^x exp(u) du \\
            & = \frac{1}{2} [exp(u)]_{-\infty}^x \\
            & = \frac{1}{2} exp(x)
        \end{split}
    \end{align}
\end{subequations}

L'expression des réciproques de $G_+$ et $G_-$ est obtenu via le développement \@ref(eq:ex1-2-2).

\begin{subequations} (\#eq:ex1-2-2)
    \begin{align}
        \begin{split}
            & G_+(x) = 1 - \frac{1}{2} exp(-x) \\
            \Leftrightarrow & \: exp(-x) = 2(1-G_+(x)) \\
            \Leftrightarrow & \: x = -ln(2(1-G_+(x))) \\
            \Rightarrow & \qquad \forall y \in [0.5, 1], \qquad G_+^{-1}(y) = -ln(2(1-y))
        \end{split} \\
        \begin{split}
            & G_-(x) = \frac{1}{2} exp(x) \\
            \Leftrightarrow & \: exp(x) = 2G_-(x) \\
            \Leftrightarrow & \: x = ln(2G_-(x)) \\
            \Rightarrow & \qquad \forall y \in [0, 0.5], \qquad G_-^{-1}(y) = ln(2y)
        \end{split}
    \end{align}
\end{subequations}

Munis de ces expressions, nous pouvons implémenter en **R** la fonction de tirage suivante :

```{r, 1.2, echo = T}
rg <- function (n) { # retourne les réalisations de g
  Un <- runif(n, min = 0, max = 1) # tirage selon la loi U[0,1]
  Gn <- sapply(Un, function (u) { # tirage selon g via sa fonction quantile
    if (u > 0.5) {
      -log(2*(1-u))
    } else {
      log(2*u)
    }
  })
}
```

3.

Nous avons utilisé notre procédure d'inversion pour générer un échantillon de taille 1000 selon la densité $g$. La représentation proposée sur la figure \@ref(fig:plot-1-g-densities) nous permet de valider graphiquement cette procédure, étant donné que la densité empirique de l'échantillon (tracée en bleu) est semblable à la densité théorique (en noir).

```{r, 1.3}
```

```{r plot-1-g-densities, fig.cap = "Comparaison de la densité théorique (en noir) et de la densité empirique (en bleu) de g"}
ggplot(data.frame(x = rg(1000)), aes(x)) +
  xlim(-5, 5) +
  geom_density(aes(color = "g_hat")) +
  geom_function(fun = g, aes(color = "g(x)")) +
  labs(x = "x", y = "Densité", color = "") +
  scale_color_manual(values = c("g(x)" = "black",
                                "g_hat" = "blue"))
```

4.

```{r, 1.4}
M <- sqrt(2 * exp(1) / pi)
```

```{r plot-1-maj, fig.cap = "Représentation de $M = \\sqrt{\\frac{2e}{\\pi}}$ plus petit majorant tel que $f(x) \\leq Mg(x)$"}
ggplot() +
  xlim(-5, 5) +
  geom_function(fun = dnorm, aes(color = "f(x)")) +
  geom_function(fun = function (x) {g(x) * M}, aes(color = "M*g(x)")) +
  geom_function(fun = function (x) {g(x) * M - dnorm(x)}, aes(color = "M*g(x)-f(x)")) +
  labs(x = "x", y = "y", color = "") +
  scale_color_manual(values = c("f(x)" = "black",
                                "M*g(x)" = "blue",
                                "M*g(x)-f(x)" = "red"))
```

La figure \@ref(fig:plot-1-maj) illustre bien que $M = \sqrt{\frac{2e}{\pi}}$ est le plus petit majorant tel que $f(x) \leq Mg(x), \: \forall x$ puisque nous pouvons voir que la courbe rouge, représentant la différence entre $Mg(x)$ et $f(x)$, est tangente à l'axe des abscisses en deux points et positive partout ailleurs.

Cette observation graphique est retrouvée par le calcul selon le développement \@ref(eq:ex1-4).

\begin{subequations} (\#eq:ex1-4)
    \begin{align}
        \begin{split}
            \forall x \in \mathbb{R}, \qquad \qquad & f(x) \leq Mg(x) \\
            \Leftrightarrow \: & M \geq \frac{f(x)}{g(x)} \qquad \qquad \qquad \qquad (g > 0)
        \end{split} \\
        \begin{split}
            \forall x \in \mathbb{R}^+, \qquad \qquad & \frac{f(x)}{g(x)} =
                                        \sqrt{\frac{2}{\pi}}exp(\frac{-x^2}{2}+x)
                                        \stackrel{\text{not}}{=} M_+(x) \\
            \Rightarrow \: & M_+^{'}(x) = \sqrt{\frac{2}{\pi}}exp(\frac{-x^2}{2}+x)(-x+1) \\
            donc, \: & M_+^{'}(x) = 0 \\
            \Leftrightarrow \: & x = 1 \\
            \Rightarrow \: & \min_{x \in \mathbb{R}^+} M_+(x) = M_+(1) = \sqrt{\frac{2e}{\pi}}
        \end{split} \\
        \begin{split}
            \forall x \in \mathbb{R}^-, \qquad \qquad & \frac{f(x)}{g(x)} =
                                        \sqrt{\frac{2}{\pi}}exp(\frac{-x^2}{2}-x)
                                        \stackrel{\text{not}}{=} M_-(x) \\
            \Rightarrow \: & M_-^{'}(x) = \sqrt{\frac{2}{\pi}}exp(\frac{-x^2}{2}+x)(-x-1) \\
            donc, \: & M_-^{'}(x) = 0 \\
            \Leftrightarrow \: & x = -1 \\
            \Rightarrow \: & \min_{x \in \mathbb{R}^-} M_-(x) = M_-(-1) = \sqrt{\frac{2e}{\pi}}
        \end{split}
    \end{align}
\end{subequations}

5.

Nous avons donc implémenté la procédure de rejet suivante en utilisant la densité $g$ et le majorant $M$ obtenus précédemment :

```{r, 1.5, echo = T}
rf <- function (n) { # retourne les réalisations de f et le taux de rejet
  Xn <- NULL
  rejected <- 0
  while (length(Xn) < n) { # on veut n réalisations
    X0 <- rg(n - length(Xn)) # génération du nombre de réalisations manquantes selon g
    U0 <- sapply(X0, function (x) runif(1, 0, M*g(x))) # tirages dans U[0, g(x0)]
    idx <- U0 <= dnorm(X0) # f=dnorm
    Xn <- c(Xn, X0[idx]) # on conserve les x0 inférieurs ou égaux à f(x0)
    rejected <- rejected + sum(1-idx) # maj du nombre de rejets
  }
  list(Xn = Xn, taux = rejected/(rejected+n))
}
```

L'idée générale est de tirer une réalisation $x_0$ selon $g$ (sur la courbe noire de la figure \@ref(fig:plot-1-g-densities)), puis de tirer une réalisation $u_0$ selon la loi $\mathcal{U}([0, M*g(x_0)])$ (entre l'axe des abscisses et la courbe bleu de la figure \@ref(fig:plot-1-maj)) et de conserver $x_0$ si $u_0 \leq f(x_0)$ (entre l'axe des abscisses et la courbe noire de la figure \@ref(fig:plot-1-maj)).

```{r}
rf.res <- rf(1000)
```

De même que pour la procédure d'inversion, nous avons généré un échantillon de taille 1000 selon la densité $f$ via notre méthode de rejet. La figure \@ref(fig:plot-1-f-densities) ci-dessous nous permet de constater que la densité empirique obtenue (tracée en bleu) est très proche de la densité théorique (représentée en noir), et donc d'attester de la pertinence de notre implémentation.

```{r plot-1-f-densities, fig.cap = "Comparaison de la densité théorique (en noir) et de la densité empirique (en bleu) de f"}
ggplot(data.frame(x = rf.res$Xn), aes(x)) +
  xlim(-5, 5) +
  geom_density(aes(color = "f_hat")) +
  geom_function(fun = dnorm, aes(color = "f(x)")) +
  labs(x = "x", y = "Densité", color = "") +
  scale_color_manual(values = c("f(x)" = "black",
                                "f_hat" = "blue"))
```

Nous obtenons un taux de rejet de `r round(rf.res$taux, 2)`. Au premier abord, ce taux peut paraitre étonnamment élevé étant donné que sur la figure \@ref(fig:plot-1-maj), la fonction $Mg(x)$ semble assez proche de $f(x)$. Cependant, il faut aussi noter que l'écart entre $Mg(x)$ et $f(x)$ est le plus important pour les valeurs les plus probables de la densité $f$. Aussi, si nous souhaitions diminuer le taux de rejet, nous pourrions considérer l'utilisation d'une densité auxiliaire permettant de mieux approximer $f$ autour de $0$, là où sa probabilité est la plus grande.

\newpage

# Exercice 2

Afin de mesurer la puissance du test permettant d'attester du niveau de performance d'un classifieur par une approche Monte Carlo, nous avons généré $M=10000$ *n*-échantillons représentant le succès ou non de la classification selon une loi $\mathcal{B}(n, p_1)$, où $p_1=0.95$ est le taux de bonne classification observé par validation croisée. Pour chacun de ces *n*-échantillons, nous calculons le taux empirique de bonne classification (sa moyenne empirique $\bar{X_n}$) et nous déterminons s'il convient de rejeter l'hypothèse nulle du test (que le taux de bonne classification est égale à $p_0$) en comparant la statistique de test calculée avec $\bar{X_n}$ au seuil de rejet au niveau $\alpha=0.05$.

```{r, Exercice2}
M <- 10000
alpha <- .05
treshold <- qnorm(1-alpha) # one-sided test
P1 <- .95
P0 <- c(0.8, 0.85, 0.9, 0.92, 0.93)
N <- seq(50, 500, by = 50)
```

Nous avons sous l'hypothèse nulle :
$$\frac{\bar{X_n} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}} \sim \mathcal{N}(0,1)$$
Nous utiliserons donc la partie gauche de l'expression comme notre statistique de test. Et comme nous avons $p_1 > p_0$, nous utilisons comme seuil de rejet $q_{1-\alpha}$, le quantile $1-\alpha$ de la loi normale centrée-réduite. La figure \@ref(fig:plot-2-region) permet de visualiser la région de rejet du test.

```{r plot-2-region, fig.cap = "Représentation de la région de rejet du test (en orange)"}
ggplot() +
  xlim(-5, 5) +
  geom_function(fun = dnorm) +
  stat_function(fun = dnorm, xlim = c(treshold, 5), fill = "orange", geom = "area") +
  geom_vline(xintercept = treshold, color = "orange", linetype = "dashed") +
  labs(x = "x", y = "Densité")
```

```{r}
p.sd <- function (p, n) { # écart-type de p sous H0
  sqrt(p * (1 - p) / n)
}
p1.r <- function (m, n) { # génére m taux à partir de n-échantillons sous H1
  apply(matrix(data = rbinom(m*n, 1, P1), nrow = m), 1, mean)
}
p.T <- function (p, p0, n) { # statistique de test
  (p - p0) / p.sd(p0, n)
}

p.MC <- function (p0, n) { # procédure MC
  pi <- p1.r(M, n) # M taux sous H1
  ti <- p.T(pi, p0, n) # stat de test pour les M taux
  mean(ti > treshold) # puissance du test
}
```

```{r}
mat <- do.call(cbind, lapply(N, function (n) { # pour chaque n
  p <- sapply(P0, function (p0) p.MC(p0, n)) # et pour chaque p0, on calcule la puissance du test
  names(p) <- P0
  p
}))
```

L'objectif étant de choisir la taille du jeu de validation permettant de démontrer la performance du classifieur, nous avons calculé la puissance de détecter un taux de bonne classification de $p_1=0.95$ contre un taux potentiel de bonne classification $p_0 \in {0.8, 0.85, 0.9, 0.92, 0.93}$ en faisant varier la taille du jeu de validation $n \in [50, 500]$ par pas de $50$.

```{r plot-2-puissances, fig.cap = "Evolution de la puissance du test selon la taille de l'échantillon, pour différentes valeurs de $p_0$"}
df <- data.frame(data = mat, row.names = P0)
colnames(df) <- N
tbl <- as_tibble(df)
tbl$P0 <- as.factor(P0)
tbl <- tbl %>% pivot_longer(!P0, names_to = "N", values_to = "pow")
tbl$N <- as.numeric(tbl$N)

ggplot(data = tbl) +
  geom_line(aes(x = N, y = pow, color = P0)) +
  labs(x = "Taille de l'échantillon", y = "Puissance du test", color = "P0")
```

La figure \@ref(fig:plot-2-puissances) présente les résultats obtenus. Nous pouvons observer que le classifieur permettrait de rejeter l'hypothèse d'un taux de bonne classification égale à $0.8$ avec une puissance de $1$ (donc une certitude de $100\%$) à partir d'un jeu de validation de taille $100$. En revanche, rejeter l'hypothèse d'un taux à $0.93$ avec une grande puissance statistique sera impossible, même avec un jeu de validation de taille $500$. S'il est primordial d'avoir une grande confiance dans le fait que le taux du classifieur est supérieur à $0.93$ il faudra donc beaucoup plus de données. Si un taux de classification à $0.9$ est acceptable, alors choisir un jeu de validation de taille $300$ permettra d'avoir une bonne confiance dans les performances attendues du classifieur, étant donné que la puissance statistique du test est alors supérieur à $0.9$.

\newpage

# Exercice 3

```{r}
load("exo3.Rdata")
```

```{r}
roc.1 <- roc(y, prob1, quiet = T)
roc.2 <- roc(y, prob2, quiet = T)
```

```{r}
auc <- function (cases, controls) {
  table <- do.call(rbind, lapply(cases, function (c) {
    row <- as.numeric(controls < c)
    row[controls == c] <- 1/2
    row
  }))
  mean(table)
}

cases.idx <- which(y == 1)
controls.idx <- which(y == -1)
cases.1 <- prob1[cases.idx]
cases.2 <- prob2[cases.idx]
controls.1 <- prob1[controls.idx]
controls.2 <- prob2[controls.idx]
auc.1 <- auc(cases.1, controls.1)
auc.2 <- auc(cases.2, controls.2)
```

```{r}
ggroc(list(roc.1 = roc.1, roc.2 = roc.2)) +
  labs(x = "Spécificité", y = "Sensibilité", color = "Classifieur") +
  scale_color_discrete(labels = c(1, 2))
```

```{r}
B <- 1000

auc.b <- function (scores) {
  replicate(B, {
    cases <- sample(scores[cases.idx], replace = T)
    controls <- sample(scores[controls.idx], replace = T)
    auc(cases, controls)
  })
}

auc.df <- data.frame(c1 = auc.b(prob1), c2 = auc.b(prob2))
colnames(auc.df) <- 1:2
```

```{r}
alpha <- .05

auc.conf.int.per <- do.call(
  cbind,
  apply(auc.df, 2, function (auc) {
    list(inf = quantile(auc, alpha/2)[[1]],
         sup = quantile(auc, 1-alpha/2)[[1]])
}))

auc.conf.int.piv <- do.call(
  cbind,
  apply(auc.df, 2, function (auc) {
    auc.hat <- mean(auc)
    list(inf = 2*auc.hat - quantile(auc, 1-alpha/2)[[1]],
         sup = 2*auc.hat - quantile(auc, alpha/2)[[1]])
}))
```

```{r}
auc.tbl <- as_tibble(t(auc.df))
auc.tbl$est <- as.factor(1:2)
auc.tbl <- auc.tbl %>% pivot_longer(!est, values_to = "auc")

auc.conf.int.per.df <- data.frame(data = t(auc.conf.int.per), row.names = 1:2)
colnames(auc.conf.int.per.df) <- c("inf", "sup")
auc.conf.int.per.tbl <- as_tibble(auc.conf.int.per.df)
auc.conf.int.per.tbl$est <- as.factor(1:2)
auc.conf.int.per.tbl <- auc.conf.int.per.tbl %>% pivot_longer(!est, values_to = "borne")
auc.conf.int.per.tbl$method <- "percentiles"
auc.conf.int.per.tbl$borne <- as.numeric(auc.conf.int.per.tbl$borne)

auc.conf.int.piv.df <- data.frame(data = t(auc.conf.int.piv), row.names = 1:2)
colnames(auc.conf.int.piv.df) <- c("inf", "sup")
auc.conf.int.piv.tbl <- as_tibble(auc.conf.int.piv.df)
auc.conf.int.piv.tbl$est <- as.factor(1:2)
auc.conf.int.piv.tbl <- auc.conf.int.piv.tbl %>% pivot_longer(!est, values_to = "borne")
auc.conf.int.piv.tbl$method <- "pivot"
auc.conf.int.piv.tbl$borne <- as.numeric(auc.conf.int.piv.tbl$borne)

auc.conf.int.tbl <- rbind(auc.conf.int.piv.tbl, auc.conf.int.per.tbl)

ggplot(data = auc.tbl, aes(x = auc, fill = est)) +
  geom_histogram(alpha = 0.6, position = "identity") +
  geom_vline(data = auc.conf.int.tbl, aes(xintercept = borne, color = est, linetype = method)) +
  labs(x = "AUC", y = "Effectif", fill = "Classifieur", color = "Classifieur", linetype = "Intervalle")
```

```{r}
P <- 1000

auc.sd <- function (auc) { # écart-type de l'AUC
  sqrt(auc * (1 - auc)) # pas besoin de /n, constante
}

d0 <- abs(auc.1 - auc.2) / (auc.sd(auc.1) + auc.sd(auc.2)) # pas besoin de /2, constante
cases.Z <- c(cases.1, cases.2)
controls.Z <- c(controls.1, controls.2)
auc.diff.perm <- replicate(P, {
  cases.i <- sample(length(cases.Z), length(cases.1))
  controls.i <- sample(length(controls.Z), length(controls.1))
  cases.x <- cases.Z[cases.i]
  cases.y <- cases.Z[-cases.i]
  controls.x <- controls.Z[controls.i]
  controls.y <- controls.Z[-controls.i]
  auc.x <- auc(cases.x, controls.x)
  auc.y <- auc(cases.y, controls.y)
  abs(auc.x - auc.y) / (auc.sd(auc.x) + auc.sd(auc.y))
})

auc.diff.perm <- c(auc.diff.perm, d0)
p.val.perm <- mean(auc.diff.perm >= d0)
```

\newpage

# Exercice 4

```{r}
load("exo4.Rdata")
```

```{r}
train.df <- data.frame(X.train)
train.df$cat <- y.train
train.df$set <- "train"
test.df <- data.frame(X.test)
test.df$cat <- y.test
test.df$set <- "test"
df <- rbind(train.df, test.df)
df$cat <- as.factor(df$cat)

ggplot(data = df) +
  geom_point(aes(x = V1, y = V2, col = cat, shape = set)) +
  scale_shape_manual(values = c(3, 19)) +
  labs(color = "Catégorie", shape = "Jeu")
```

```{r}
K <- c(1, 3, 5, 7, 9, 11)
rate <- sapply(K, function (k) {
  pred.test <- knn(X.train, X.test, y.train, k)
  mean(pred.test == y.test)
})
```

```{r}
B <- 100

K <- c(1, 3, 5, 7, 9, 11)
pred.b <- lapply(K, function (k) {
  replicate(B, {
      idx <- sample(length(y.train), replace = T)
      as.numeric(knn(X.train[idx,], X.test, y.train[idx], k)) - 1
  })
})
rate.b <- sapply(pred.b, function (p) {
  p.maj <- as.numeric(apply(p, 1, mean) >= .5 )
  mean(p.maj == y.test)
})
rate.ind <- do.call(cbind, lapply(pred.b, function (p) {
  apply(p == y.test, 2, mean)
}))
```

```{r}
rate.df <- data.frame(knn = rate, bagging = rate.b)
rate.tbl <- as_tibble(rate.df)
rate.tbl$K <- K
rate.tbl <- rate.tbl %>% pivot_longer(!K, names_to = "method", values_to = "rate")

rate.ind.df <- data.frame(t(rate.ind))
colnames(rate.ind.df) <- as.factor(1:B)
rate.ind.tbl <- as_tibble(rate.ind.df)
rate.ind.tbl$K <- K
rate.ind.tbl <- rate.ind.tbl %>% pivot_longer(!K, names_to = "B", values_to = "rate")

ggplot() +
  geom_line(data = rate.ind.tbl, aes(x = K, y = rate, group = B, col = "bagging"), alpha = .1) +
  geom_line(data = rate.tbl, aes(x = K, y = rate, col = method)) +
  scale_color_discrete(labels = c("Bagging", "k-ppv")) +
  labs(x = "k", y = "Taux de bonne classification", col = "Méthode")
```
